{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/losangeletimes/Oasis-Kittens/blob/main/copy_of_gemini_api_key_setup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tJONwo09tS3"
      },
      "source": [
        "# Setup\n",
        "\n",
        "Please ensure you have imported a Gemini API key from AI Studio.\n",
        "You can do this directly in the Secrets tab on the left.\n",
        "\n",
        "After doing so, please run the setup cell below."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To run this code you need to install the following dependencies:\n",
        "# pip install google-genai\n",
        "\n",
        "import base64\n",
        "import os\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "\n",
        "def generate():\n",
        "    client = genai.Client(\n",
        "        api_key=os.environ.get(\"GEMINI_API_KEY\"),\n",
        "    )\n",
        "\n",
        "    model = \"gemini-2.5-pro\"\n",
        "    contents = [\n",
        "        types.Content(\n",
        "            role=\"user\",\n",
        "            parts=[\n",
        "                types.Part.from_text(text=\"\"\"# To run this code you need to install the following dependencies:\n",
        "# pip install google-genai\n",
        "\n",
        "import base64\n",
        "import os\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "\n",
        "def generate():\n",
        "    client = genai.Client(\n",
        "        api_key=os.environ.get(\\\"GEMINI_API_KEY\\\"),\n",
        "    )\n",
        "\n",
        "    model = \\\"gemini-2.5-pro\\\"\n",
        "    contents = [\n",
        "        types.Content(\n",
        "            role=\\\"user\\\",\n",
        "            parts=[\n",
        "                types.Part.from_text(text=\\\"\\\"\\\"INSERT_INPUT_HERE\\\"\\\"\\\"),\n",
        "            ],\n",
        "        ),\n",
        "    ]\n",
        "    generate_content_config = types.GenerateContentConfig(\n",
        "        thinking_config = types.ThinkingConfig(\n",
        "            thinking_budget=-1,\n",
        "        ),\n",
        "        response_mime_type=\\\"text/plain\\\",\n",
        "    )\n",
        "\n",
        "    for chunk in client.models.generate_content_stream(\n",
        "        model=model,\n",
        "        contents=contents,\n",
        "        config=generate_content_config,\n",
        "    ):\n",
        "        print(chunk.text, end=\\\"\\\")\n",
        "\n",
        "if __name__ == \\\"__main__\\\":\n",
        "    generate()\n",
        "\"\"\"),\n",
        "            ],\n",
        "        ),\n",
        "        types.Content(\n",
        "            role=\"model\",\n",
        "            parts=[\n",
        "                types.Part.from_text(text=\"\"\"**Defining the Task Prompt**\n",
        "\n",
        "I'm working on a prompt that showcases the Gemini 2.5 Pro model's new \\\"thinking\\\" feature. Currently, the main focus is on crafting a prompt that necessitates a demonstration of the model's reasoning before providing a final answer. This involves replacing the `INSERT_INPUT_HERE` placeholder with an appropriate task for the model. The aim is for the prompt to effectively illustrate the model's step-by-step thought process.\n",
        "\n",
        "\n",
        "**Formulating a Reasoning Task**\n",
        "\n",
        "I've been iterating on the task prompt, focusing on complexity. The logic puzzle concept feels promising, as it forces a step-by-step reasoning demonstration. I'm currently refining the constraints to ensure the solution necessitates multiple logical deductions, not just simple retrieval. I'm also considering adding an explicit request for the model to show its thought process.\n",
        "\n",
        "\n",
        "**Choosing the Best Prompt**\n",
        "\n",
        "I've settled on the logic puzzle as the optimal choice. It's a classic format ideal for showcasing the \\\"thinking\\\" feature. Now, my focus is on refining its presentation for the model. I'm ensuring the clues are clear, concise, and structured logically to facilitate the step-by-step reasoning Gemini 2.5 Pro will demonstrate. My goal is to maximize the visual impact of the model's thought process.\n",
        "\n",
        "\n",
        "**Finalizing the Puzzle Prompt**\n",
        "\n",
        "I've incorporated the logic puzzle into the code, replacing the placeholder with the prompt. The formatting is set, and it's all ready for testing. I've ensured proper multiline string handling with triple quotes. I'm satisfied that this setup effectively illustrates the \\\"thinking\\\" capabilities. Now, the main task is to observe the model's output to verify the demonstration is successful.\n",
        "\n",
        "\n",
        "\"\"\"),\n",
        "                types.Part.from_text(text=\"\"\"```python\n",
        "# To run this code you need to install the following dependencies:\n",
        "# pip install google-genai\n",
        "\n",
        "import base64\n",
        "import os\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "\n",
        "def generate():\n",
        "    \\\"\\\"\\\"\n",
        "    Generates content from the Gemini 2.5 Pro model using a streaming request.\n",
        "\n",
        "    This function demonstrates how to use the \\\"thinking\\\" feature, which allows\n",
        "    the model to show its thought process before providing the final answer.\n",
        "    A logic puzzle is used as the input to trigger this multi-step reasoning.\n",
        "\n",
        "    Ensure your GEMINI_API_KEY environment variable is set.\n",
        "    \\\"\\\"\\\"\n",
        "    try:\n",
        "        client = genai.Client(\n",
        "            api_key=os.environ.get(\\\"GEMINI_API_KEY\\\"),\n",
        "        )\n",
        "    except AttributeError:\n",
        "        print(\\\"Error: The GEMINI_API_KEY environment variable is not set.\\\")\n",
        "        print(\\\"Please set it before running the script.\\\")\n",
        "        print(\\\"Example: export GEMINI_API_KEY='your_api_key_here'\\\")\n",
        "        return\n",
        "\n",
        "\n",
        "    model = \\\"gemini-1.5-pro-latest\\\" # Using a stable, widely available model\n",
        "    contents = [\n",
        "        types.Content(\n",
        "            role=\\\"user\\\",\n",
        "            parts=[\n",
        "                types.Part.from_text(text=\\\"\\\"\\\"Solve this logic puzzle and explain your reasoning step-by-step.\n",
        "\n",
        "There are three people: Sarah, Mike, and Tom.\n",
        "They each have a different favorite color: Blue, Green, or Red.\n",
        "They each have a different pet: a Dog, a Cat, or a Fish.\n",
        "\n",
        "Here are the clues:\n",
        "1. Mike does not like the color Red.\n",
        "2. The person who likes Blue has a Dog.\n",
        "3. Tom has a Fish.\n",
        "4. Sarah's favorite color is not Green.\n",
        "\n",
        "Based on these clues, determine each person's favorite color and pet.\\\"\\\"\\\"),\n",
        "            ],\n",
        "        ),\n",
        "    ]\n",
        "    # The thinking_config enables the model to output its thought process.\n",
        "    # A thinking_budget of -1 allows for an unlimited thought process.\n",
        "    generate_content_config = types.GenerateContentConfig(\n",
        "        thinking_config = types.ThinkingConfig(\n",
        "            thinking_budget=-1,\n",
        "        ),\n",
        "        response_mime_type=\\\"text/plain\\\",\n",
        "    )\n",
        "\n",
        "    print(\\\"--- Model's thought process and final answer ---\\\\n\\\")\n",
        "    try:\n",
        "        for chunk in client.models.generate_content_stream(\n",
        "            model=model,\n",
        "            contents=contents,\n",
        "            config=generate_content_config,\n",
        "        ):\n",
        "            print(chunk.text, end=\\\"\\\")\n",
        "    except Exception as e:\n",
        "        print(f\\\"\\\\nAn error occurred while generating content: {e}\\\")\n",
        "\n",
        "    print(\\\"\\\\n\\\\n--- End of generation ---\\\")\n",
        "\n",
        "\n",
        "if __name__ == \\\"__main__\\\":\n",
        "    generate()\n",
        "```\"\"\"),\n",
        "            ],\n",
        "        ),\n",
        "        types.Content(\n",
        "            role=\"user\",\n",
        "            parts=[\n",
        "                types.Part.from_text(text=\"\"\"INSERT_INPUT_HERE\"\"\"),\n",
        "            ],\n",
        "        ),\n",
        "    ]\n",
        "    generate_content_config = types.GenerateContentConfig(\n",
        "        thinking_config = types.ThinkingConfig(\n",
        "            thinking_budget=-1,\n",
        "        ),\n",
        "        response_mime_type=\"text/plain\",\n",
        "    )\n",
        "\n",
        "    for chunk in client.models.generate_content_stream(\n",
        "        model=model,\n",
        "        contents=contents,\n",
        "        config=generate_content_config,\n",
        "    ):\n",
        "        print(chunk.text, end=\"\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    generate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 671
        },
        "id": "QVpQWRop9wV3",
        "outputId": "cd321656-0b47-4c1a-8d45-361c5eec34e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Missing key inputs argument! To use the Google AI API, provide (`api_key`) arguments. To use the Google Cloud API, provide (`vertexai`, `project` & `location`) arguments.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-6-2877970901.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m     \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-6-2877970901.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     client = genai.Client(\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GEMINI_API_KEY\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/genai/client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vertexai, api_key, credentials, project, location, debug_config, http_options)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0mhttp_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHttpOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m     self._api_client = self._get_api_client(\n\u001b[0m\u001b[1;32m    220\u001b[0m         \u001b[0mvertexai\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvertexai\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/genai/client.py\u001b[0m in \u001b[0;36m_get_api_client\u001b[0;34m(vertexai, api_key, credentials, project, location, debug_config, http_options)\u001b[0m\n\u001b[1;32m    263\u001b[0m       )\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m     return BaseApiClient(\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0mvertexai\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvertexai\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/genai/_api_client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vertexai, api_key, credentials, project, location, http_options)\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Implicit initialization or missing arguments.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    532\u001b[0m             \u001b[0;34m'Missing key inputs argument! To use the Google AI API,'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m             \u001b[0;34m' provide (`api_key`) arguments. To use the Google Cloud API,'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Missing key inputs argument! To use the Google AI API, provide (`api_key`) arguments. To use the Google Cloud API, provide (`vertexai`, `project` & `location`) arguments."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qn95PgCj9tS-"
      },
      "source": [
        "!pip install -U -q \"google\"\n",
        "!pip install -U -q \"google.genai\"\n",
        "\n",
        "import os\n",
        "from google.colab import userdata\n",
        "from google.colab import drive\n",
        "os.environ[\"GEMINI_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\")\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "# Please ensure that uploaded files are available in the AI Studio folder or change the working folder.\n",
        "os.chdir(\"/content/drive/MyDrive/Google AI Studio\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAT4uilL9tTB"
      },
      "source": [
        "# Generated Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgWZUhce9tTD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 671
        },
        "outputId": "d7ccfb3e-96b3-41e4-be7e-d618c40e0854"
      },
      "source": [
        "# To run this code you need to install the following dependencies:\n",
        "# pip install google-genai\n",
        "\n",
        "import base64\n",
        "import os\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "\n",
        "def generate():\n",
        "    client = genai.Client(\n",
        "        api_key=os.environ.get(\"GEMINI_API_KEY\"),\n",
        "    )\n",
        "\n",
        "    model = \"gemini-2.5-pro\"\n",
        "    contents = [\n",
        "        types.Content(\n",
        "            role=\"user\",\n",
        "            parts=[\n",
        "                types.Part.from_text(text=\"\"\"# To run this code you need to install the following dependencies:\n",
        "# pip install google-genai\n",
        "\n",
        "import base64\n",
        "import os\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "\n",
        "def generate():\n",
        "    client = genai.Client(\n",
        "        api_key=os.environ.get(\\\"GEMINI_API_KEY\\\"),\n",
        "    )\n",
        "\n",
        "    model = \\\"gemini-2.5-pro\\\"\n",
        "    contents = [\n",
        "        types.Content(\n",
        "            role=\\\"user\\\",\n",
        "            parts=[\n",
        "                types.Part.from_text(text=\\\"\\\"\\\"INSERT_INPUT_HERE\\\"\\\"\\\"),\n",
        "            ],\n",
        "        ),\n",
        "    ]\n",
        "    generate_content_config = types.GenerateContentConfig(\n",
        "        thinking_config = types.ThinkingConfig(\n",
        "            thinking_budget=-1,\n",
        "        ),\n",
        "        response_mime_type=\\\"text/plain\\\",\n",
        "    )\n",
        "\n",
        "    for chunk in client.models.generate_content_stream(\n",
        "        model=model,\n",
        "        contents=contents,\n",
        "        config=generate_content_config,\n",
        "    ):\n",
        "        print(chunk.text, end=\\\"\\\")\n",
        "\n",
        "if __name__ == \\\"__main__\\\":\n",
        "    generate()\n",
        "\"\"\"),\n",
        "            ],\n",
        "        ),\n",
        "        types.Content(\n",
        "            role=\"model\",\n",
        "            parts=[\n",
        "                types.Part.from_text(text=\"\"\"**Defining the Task Prompt**\n",
        "\n",
        "I'm working on a prompt that showcases the Gemini 2.5 Pro model's new \\\"thinking\\\" feature. Currently, the main focus is on crafting a prompt that necessitates a demonstration of the model's reasoning before providing a final answer. This involves replacing the `INSERT_INPUT_HERE` placeholder with an appropriate task for the model. The aim is for the prompt to effectively illustrate the model's step-by-step thought process.\n",
        "\n",
        "\n",
        "**Formulating a Reasoning Task**\n",
        "\n",
        "I've been iterating on the task prompt, focusing on complexity. The logic puzzle concept feels promising, as it forces a step-by-step reasoning demonstration. I'm currently refining the constraints to ensure the solution necessitates multiple logical deductions, not just simple retrieval. I'm also considering adding an explicit request for the model to show its thought process.\n",
        "\n",
        "\n",
        "**Choosing the Best Prompt**\n",
        "\n",
        "I've settled on the logic puzzle as the optimal choice. It's a classic format ideal for showcasing the \\\"thinking\\\" feature. Now, my focus is on refining its presentation for the model. I'm ensuring the clues are clear, concise, and structured logically to facilitate the step-by-step reasoning Gemini 2.5 Pro will demonstrate. My goal is to maximize the visual impact of the model's thought process.\n",
        "\n",
        "\n",
        "**Finalizing the Puzzle Prompt**\n",
        "\n",
        "I've incorporated the logic puzzle into the code, replacing the placeholder with the prompt. The formatting is set, and it's all ready for testing. I've ensured proper multiline string handling with triple quotes. I'm satisfied that this setup effectively illustrates the \\\"thinking\\\" capabilities. Now, the main task is to observe the model's output to verify the demonstration is successful.\n",
        "\n",
        "\n",
        "\"\"\"),\n",
        "                types.Part.from_text(text=\"\"\"```python\n",
        "# To run this code you need to install the following dependencies:\n",
        "# pip install google-genai\n",
        "\n",
        "import base64\n",
        "import os\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "\n",
        "def generate():\n",
        "    \\\"\\\"\\\"\n",
        "    Generates content from the Gemini 2.5 Pro model using a streaming request.\n",
        "\n",
        "    This function demonstrates how to use the \\\"thinking\\\" feature, which allows\n",
        "    the model to show its thought process before providing the final answer.\n",
        "    A logic puzzle is used as the input to trigger this multi-step reasoning.\n",
        "\n",
        "    Ensure your GEMINI_API_KEY environment variable is set.\n",
        "    \\\"\\\"\\\"\n",
        "    try:\n",
        "        client = genai.Client(\n",
        "            api_key=os.environ.get(\\\"GEMINI_API_KEY\\\"),\n",
        "        )\n",
        "    except AttributeError:\n",
        "        print(\\\"Error: The GEMINI_API_KEY environment variable is not set.\\\")\n",
        "        print(\\\"Please set it before running the script.\\\")\n",
        "        print(\\\"Example: export GEMINI_API_KEY='your_api_key_here'\\\")\n",
        "        return\n",
        "\n",
        "\n",
        "    model = \\\"gemini-1.5-pro-latest\\\" # Using a stable, widely available model\n",
        "    contents = [\n",
        "        types.Content(\n",
        "            role=\\\"user\\\",\n",
        "            parts=[\n",
        "                types.Part.from_text(text=\\\"\\\"\\\"Solve this logic puzzle and explain your reasoning step-by-step.\n",
        "\n",
        "There are three people: Sarah, Mike, and Tom.\n",
        "They each have a different favorite color: Blue, Green, or Red.\n",
        "They each have a different pet: a Dog, a Cat, or a Fish.\n",
        "\n",
        "Here are the clues:\n",
        "1. Mike does not like the color Red.\n",
        "2. The person who likes Blue has a Dog.\n",
        "3. Tom has a Fish.\n",
        "4. Sarah's favorite color is not Green.\n",
        "\n",
        "Based on these clues, determine each person's favorite color and pet.\\\"\\\"\\\"),\n",
        "            ],\n",
        "        ),\n",
        "    ]\n",
        "    # The thinking_config enables the model to output its thought process.\n",
        "    # A thinking_budget of -1 allows for an unlimited thought process.\n",
        "    generate_content_config = types.GenerateContentConfig(\n",
        "        thinking_config = types.ThinkingConfig(\n",
        "            thinking_budget=-1,\n",
        "        ),\n",
        "        response_mime_type=\\\"text/plain\\\",\n",
        "    )\n",
        "\n",
        "    print(\\\"--- Model's thought process and final answer ---\\\\n\\\")\n",
        "    try:\n",
        "        for chunk in client.models.generate_content_stream(\n",
        "            model=model,\n",
        "            contents=contents,\n",
        "            config=generate_content_config,\n",
        "        ):\n",
        "            print(chunk.text, end=\\\"\\\")\n",
        "    except Exception as e:\n",
        "        print(f\\\"\\\\nAn error occurred while generating content: {e}\\\")\n",
        "\n",
        "    print(\\\"\\\\n\\\\n--- End of generation ---\\\")\n",
        "\n",
        "\n",
        "if __name__ == \\\"__main__\\\":\n",
        "    generate()\n",
        "```\"\"\"),\n",
        "            ],\n",
        "        ),\n",
        "        types.Content(\n",
        "            role=\"user\",\n",
        "            parts=[\n",
        "                types.Part.from_text(text=\"\"\"INSERT_INPUT_HERE\"\"\"),\n",
        "            ],\n",
        "        ),\n",
        "    ]\n",
        "    generate_content_config = types.GenerateContentConfig(\n",
        "        thinking_config = types.ThinkingConfig(\n",
        "            thinking_budget=-1,\n",
        "        ),\n",
        "        response_mime_type=\"text/plain\",\n",
        "    )\n",
        "\n",
        "    for chunk in client.models.generate_content_stream(\n",
        "        model=model,\n",
        "        contents=contents,\n",
        "        config=generate_content_config,\n",
        "    ):\n",
        "        print(chunk.text, end=\"\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    generate()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Missing key inputs argument! To use the Google AI API, provide (`api_key`) arguments. To use the Google Cloud API, provide (`vertexai`, `project` & `location`) arguments.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-5-3437989220.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m     \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-5-3437989220.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     client = genai.Client(\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GEMINI_API_KEY\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/genai/client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vertexai, api_key, credentials, project, location, debug_config, http_options)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0mhttp_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHttpOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m     self._api_client = self._get_api_client(\n\u001b[0m\u001b[1;32m    220\u001b[0m         \u001b[0mvertexai\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvertexai\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/genai/client.py\u001b[0m in \u001b[0;36m_get_api_client\u001b[0;34m(vertexai, api_key, credentials, project, location, debug_config, http_options)\u001b[0m\n\u001b[1;32m    263\u001b[0m       )\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m     return BaseApiClient(\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0mvertexai\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvertexai\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/genai/_api_client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vertexai, api_key, credentials, project, location, http_options)\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Implicit initialization or missing arguments.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    532\u001b[0m             \u001b[0;34m'Missing key inputs argument! To use the Google AI API,'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m             \u001b[0;34m' provide (`api_key`) arguments. To use the Google Cloud API,'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Missing key inputs argument! To use the Google AI API, provide (`api_key`) arguments. To use the Google Cloud API, provide (`vertexai`, `project` & `location`) arguments."
          ]
        }
      ]
    }
  ]
}